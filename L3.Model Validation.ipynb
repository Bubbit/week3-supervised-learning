{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ML-Challenge/week3-supervised-learning/blob/master/L2.Model%20Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download utils.py to working directory\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/ML-Challenge/week3-supervised-learning/master/utils.py', 'utils.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:07.542630Z",
     "start_time": "2020-02-07T13:41:06.628737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import utils\n",
    "# We'll be using this module throughout the lesson\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:07.691290Z",
     "start_time": "2020-02-07T13:41:07.687738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# and setting the size of all plots.\n",
    "plt.rcParams['figure.figsize'] = [11, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train, test and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a holdout dataset as any data that is not used for training and is only used to assess model performance. The available data is split into two datasets. One used for training, and one that is simply off limits while we are training our models, called a test (or holdout) dataset.\n",
    "\n",
    "This step is vital to model validation and is the number one step we can take to ensure our model's performance. We use the holdout sample as a testing dataset so that we can have an unbiased estimate for our model's performance after we are completely done training. \n",
    "\n",
    "Generally, a good rule of thumb is using an `80:20` split.This equates to setting aside twenty percent of the data for the test set and using the rest for training. We might choose to use more training data when the overall data is limited (`90:10`), or less training data if the modeling method is computationally expensive (`70:30`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for preliminary testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the test set is off limits until we are completely done training, but what do we do when testing model parameters? For example, if we run a random forest model with 100 trees and one with 1000 trees, which dataset do we use to test these results? When testing parameters, tuning hyper-parameters, or anytime we are frequently evaluating model performance we need to create a second holdout sample, called the validation dataset.\n",
    "\n",
    "For this dataset, the available data is the original training dataset, which is then split in the same manner used to split the original complete dataset. We use the validation sample to asses our model's performance when using different parameter values.\n",
    "\n",
    "To created the both holdout samples, the testing and the validation datasets, we use scikit-learn's `train_test_split()` function twice. The first call will create training and testing datasets like normal\n",
    "\n",
    "```\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "```\n",
    "\n",
    "The second call we split this so-called temporary training dataset into the final training and validation datasets.\n",
    "\n",
    "```\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size = 0.25, random_state = 42)\n",
    "```\n",
    "\n",
    "In the above example, we used first an 80/20 split to create the test set. With the 80% training dataset, we used a 75/25 split to create a validation dataset. Leaving us with 60% of the data for training, 20% for validation, and 20% for testing.\n",
    "\n",
    "![Train, test, validation](assets/train_test_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy metrics: regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned about holdout samples let's discuss accuracy metrics used when validating models - starting with regression models. Remember, regression models are built for continuous variables. This could be predicting the number of points a player will score tomorrow, or the number of puppies a dog is about to have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean absolute error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of a regression model, we can use the mean absolute error. It is the simplest and most intuitive error metric and is the average absolute difference between the predictions $y_i$ and the actual values $\\hat{y}$:\n",
    "\n",
    "$$ MAE = \\frac{\\sum_{i=1}^n |y_i - \\hat{y}_i|}{n} $$\n",
    "\n",
    "If a dog had six puppies, but we predicted only four, the absolute difference would be two. This metric treats all points equally and is not sensitive to outliers. When dealing with applications where we don't want large errors to have a major impact, the mean absolute error can be used. And example could be predicting the car's monthly gas bill, when an outlier may have been caused by a one-time road trip.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communicating modeling results can be difficult. However, most clients people that on average, a predictive model was off by some number. This makes explaining the mean absolute error easy. For example, when predicting the number of wins for a basketball team, if we predict 42, and they end up with 40, we can easily explain that the error was two wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have two arrays. `y_test`, the true number of wins for all 30 NBA teams in 2017 and `predictions`, which contains a prediction for each team. Let's calculate the MAE both manually using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:08.730438Z",
     "start_time": "2020-02-07T13:41:08.727787Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:08.898707Z",
     "start_time": "2020-02-07T13:41:08.895257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a manual calculation, the error is 5.9\n"
     ]
    }
   ],
   "source": [
    "# Manually calculate the MAE\n",
    "n = len(utils.nba_predictions)\n",
    "mae_one = sum(abs(utils.nba_y_test - utils.nba_predictions)) / n\n",
    "print('With a manual calculation, the error is {}'.format(mae_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:09.070671Z",
     "start_time": "2020-02-07T13:41:09.067330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scikit-lean, the error is 5.9\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to calculate the MAE\n",
    "mae_two = mean_absolute_error(utils.nba_y_test, utils.nba_predictions)\n",
    "print('Using scikit-lean, the error is {}'.format(mae_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the mean squared error (MSE). It is the most widely used regression error metric for regression models. It is calculated similarly to the mean absolute error, but this time we square the difference term.\n",
    "\n",
    "$$ MSE = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n} $$\n",
    "\n",
    "The MSE allows larger errors to have a larger impact on the model. Using the previous car example, if we knew once a year we might go on a road trip, we might expect to occasionally have a large error and would want our model to pick up on these trips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the 2017 NBA predictions again. Every year, there are at least a couple of NBA teams that win way more games than expected. If you use the MAE, this accuracy metric does not reflect the bad predictions as much as if you use the MSE. Squaring the large errors from bad predictions will make the accuracy look worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:09.789808Z",
     "start_time": "2020-02-07T13:41:09.787546Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:09.972903Z",
     "start_time": "2020-02-07T13:41:09.969537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a manual calculation, the error is 49.1\n"
     ]
    }
   ],
   "source": [
    "n = len(utils.nba_predictions)\n",
    "# Finish the manual calculation of the MSE\n",
    "mse_one = sum((utils.nba_y_test - utils.nba_predictions)**2) / n\n",
    "print('With a manual calculation, the error is {}'.format(mse_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:10.154886Z",
     "start_time": "2020-02-07T13:41:10.151659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scikit-lean, the error is 49.1\n"
     ]
    }
   ],
   "source": [
    "# Use the scikit-learn function to calculate MSE\n",
    "mse_two = mean_squared_error(utils.nba_y_test, utils.nba_predictions)\n",
    "print('Using scikit-lean, the error is {}'.format(mse_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE vs MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking between the MAE and the MSE comes down to the application. These results are in different units though and should not be directly compared!\n",
    "To practice these metrics, let's use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on data subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In professional basketball, there are two conferences, the East and the West. Coaches and fans often only care about how teams in their own conference will do this year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been working on an NBA prediction model and would like to determine if the predictions were better for the East or West conference. We added a third array to the data called `utils.nba_labels`, which contains an \"E\" for the East teams, and a \"W\" for the West."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:11.290453Z",
     "start_time": "2020-02-07T13:41:11.288144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the East conference teams\n",
    "east_teams = utils.nba_labels == \"E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:11.583172Z",
     "start_time": "2020-02-07T13:41:11.580793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create arrays for the true and predicted values\n",
    "true_east = utils.nba_y_test[east_teams]\n",
    "preds_east = utils.nba_predictions[east_teams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:11.903195Z",
     "start_time": "2020-02-07T13:41:11.899669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for East teams is 6.733333333333333\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy metrics\n",
    "print('The MAE for East teams is {}'.format(mean_absolute_error(true_east, preds_east)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:12.179626Z",
     "start_time": "2020-02-07T13:41:12.176706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create arrays for the true and predicted values\n",
    "true_west = utils.nba_y_test[~east_teams]\n",
    "preds_west = utils.nba_predictions[~east_teams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:12.397623Z",
     "start_time": "2020-02-07T13:41:12.393117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for West teams is 5.066666666666666\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy metrics\n",
    "print('The MAE for West teams is {}'.format(mean_absolute_error(true_west, preds_west)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the Western conference predictions were about two games better on average. Over the past few seasons, the Western teams have generally won the same number of games as the experts have predicted. Teams in the East are just not as predictable as those in the West."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already understand classification models; now let's look at their accuracy metrics. Classification accuracy metrics are quite a bit different than regression ones. Remember, with classification models, we are predicting what category an observation falls into. There are a lot of accuracy metrics available: `precision`, `recall` (also called `sensitivity`), `accuracy`, `specificity`, `F1-Score`, and it's variations, and several others.\n",
    "\n",
    "We will focus on precision, recall, and accuracy as each of these are easy to understand and have very practical applications. One way to calculate these metrics is to use the values from the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making predictions, especially if there is a binary outcome, this matrix is one of the first outputs we should preview. When we have a binary outcome, the confusion matrix is a 2x2 matrix that shows how our predictions faired across the two outcomes. For example, for predictions of `0` that were actually `0` (or true negatives), we look at the `0,0` square of the matrix.\n",
    "\n",
    "![Confusion matrix](assets/confusion_matrix.png)\n",
    "\n",
    "All of the above accuracy metrics can be calculated using the values from this matrix, and it is a great way to visualize the initial results of our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a confusion matrix using `scikit-learn`'s function `confusion_,matrix()`. When dealing with binary data, this will print out a 2x2 array which represents the confusion matrix. In this matrix, the row index represents the true category, and the column index represents the predicted category. Therefore, the 1,0 entry of the array represents the number of true `1s` that were predicted to be `0`, or 8 in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the easiest metric to understand and represents the overall ability of the model to correctly predict the correct classification. Using the confusion matrix, we add the values were predicted `0` and are actually `0` (which are called true negatives), to the values predicted to be `1` that are `1` (called true positives), and then divide by the total number of observations:\n",
    "\n",
    "$$ Accuracy = \\frac{TN + TP}{N} = \\frac{23 + 62}{23 + 7 + 8 + 62} = 0.85 $$\n",
    "\n",
    "In this case, our accuracy was 85%. In this example, we can associate a true positive as predicted 1's that are also 1's. However, if our categories were win or loss, we might associate a true positive as the number of predicted wins that were actually wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is precision or the number of true positives out of all predicted positive values:\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} = \\frac{62}{62 + 7} = 0.90 $$\n",
    "\n",
    "Precision is used when we don't want to over-predict positive values. It is cost $2,000 to fly-in potential new employee's, a company may only have on-campus interviews with individuals that they are really believe are going to join their company. In the example, almost 9 out of 10 predicted 1's would have joined the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall metric is about finding all positive values:\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP + FN} = \\frac{62}{62 + 8} = 0.885 $$\n",
    "\n",
    "Here we correctly predicted 62 true positives and had 8 false negatives. Our recall is 62 out of 70. Recall is used when we can't afford to miss any positive values. For example, even if a patient has a small chance of having cancer, we may want to give them additional tests. The cost of missing a patient who has cancer is far greater than the cost of additional screenings for that patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, precision, and recall are called similarly. Use the desired accuracy metric function and provide the true and predicted values.\n",
    "\n",
    "```\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "accuracy_score(y_test, test_predictions) # .85\n",
    "precision_score(y_test, test_predictions) # .8986\n",
    "recall_score(y_test, test_predictions) # .8857\n",
    "```\n",
    "\n",
    "Use the desired accuracy metric function and provide the true and predicted values. A single value will be produced as a result. In this example, we got the same values that we calculated using the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices, again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a confusion matrix in Python is simple. The biggest challenge will be making sure you understand the orientation of the matrix. This exercise makes sure you understand the `sklearn` implementation of confusion matrices. Here, we have created a `classifier` model using the `tic_tac_toe` dataset to predict outcomes of 0 (loss) or 1 (a win) for Player One."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:16.966002Z",
     "start_time": "2020-02-07T13:41:16.888294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[113 191]\n",
      " [  7 552]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create predictions\n",
    "test_predictions = utils.classifier.predict(utils.X_test)\n",
    "\n",
    "# Create and print the confusion matrix\n",
    "cm = confusion_matrix(utils.y_test, test_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:41:17.275830Z",
     "start_time": "2020-02-07T13:41:17.273005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of true positives is: 552\n"
     ]
    }
   ],
   "source": [
    "# Print the true positives (actual 1s that were predicted 1s)\n",
    "print(\"The number of true positives is: {}\".format(cm[1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision vs. recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metrics we use to evaluate our model should always be based on the specific application. For this example, let's assume we are a really sore loser when it comes to playing Tic-Tac-Toe, but only when we are certain that we are going to win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose the most appropriate accuracy metric, either precision or recall, to complete this example. But remember, if we think we are going to win, we better win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:42:59.369533Z",
     "start_time": "2020-02-07T13:42:59.367112Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T13:43:03.450089Z",
     "start_time": "2020-02-07T13:43:03.444894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision value is 0.74\n"
     ]
    }
   ],
   "source": [
    "# Create precision or recall score based on the metric you imported\n",
    "score = precision_score(utils.y_test, test_predictions)\n",
    "\n",
    "# Print the final result\n",
    "print(\"The precision value is {0:.2f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error due to under/over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
