{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ML-Challenge/week3-supervised-learning/blob/master/L4.Tree-Based%20Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are supervised learning models used for problems involving classification and regression. Tree models present a high flexibility that comes at a price: on one hand, trees are able to capture complex non-linear relationships; on the other hand, they are prone to memorizing the noise present in a dataset. By aggregating the predictions of trees that are trained differently, ensemble methods take advantage of the flexibility of trees while reducing their tendency to memorize noise. Ensemble methods are used across a variety of fields and have a proven track record of winning many machine learning competitions. In this lesson, we'll learn how to use Python to train decision trees and tree-based models with the user-friendly scikit-learn machine learning library. We'll see the advantages and shortcomings of trees and demonstrate how ensembling can alleviate these shortcomings, all while practicing on real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T08:53:13.285548Z",
     "start_time": "2020-02-09T08:53:13.279565Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Download utils.py to working directory\n",
    "#import urllib.request\n",
    "#urllib.request.urlretrieve('https://raw.githubusercontent.com/ML-Challenge/week3-supervised-learning/master/utils.py', 'utils.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T08:53:16.299156Z",
     "start_time": "2020-02-09T08:53:13.452181Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import utils\n",
    "# We'll be using this module throughout the lesson\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T08:53:16.721145Z",
     "start_time": "2020-02-09T08:53:16.332182Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# and setting the size of all plots.\n",
    "plt.rcParams['figure.figsize'] = [11, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Classification and Regression Trees (CART) are a set of supervised learning models used for problems involving classification and regression. In this chapter, we'll introduce the CART algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Decision tree for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Classification-tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given a labeled dataset, a classification tree learns a sequence of if-else questions about individual features in order to infer the labels. In contrast to linear models, trees are able to capture non-linear relantionships between features and labels. In addition, trees don't require the features to be on the same scale through standardization for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Breast Cancer Dataset in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To understand trees more concretely, we'll try to predict whether a tumor is malignant or benign in the Wisconsing Breast Cancer dataset using only 2 features.\n",
    "![Breast Cancer](assets/breast_cancer.png)\n",
    "The figure here shows a scatterplot of two cancerous cell features with malignant-tumors in blue and benign-tumors in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Decision-tree Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When a classification tree is trained on this dataset, the tree learns a sequence of if-else questions with each question involving one feature and one split-point. Take a look at the tree diagram:\n",
    "![Tree Diagram](assets/tree_diagram.png)\n",
    "At the top, the tree asks wheter the concave-points mean of an instance is smaller than 0.051. It it is, the instance traverses the `True` branch; otherwise, it traverses the `False` branch. Similarly, the instance keeps traversing the internal branches until it reaches an end. The label of the instance is then predicted to be that of the prevailing class at that end.\n",
    "\n",
    "The maximum number of branches separating the top from an extreme-end is known as the `maximum depth` which is equal to 2 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Decision Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A classification-model divides the feature-space into regions where all instances in one region are assigned to only one class-label. These regions are known as decision-regions. Decision-regions are separated by surfaces called decision-boundaries.\n",
    "![Linear classifier decision region](assets/linear_decision.png)\n",
    "The figure above shows the decision-regions of a linear-classifier. Not how the boundary is a straight-line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Decision Regions: CART vs. Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In contrast, as shown here on the right, a classification-tree produces rectangular decision-regions in the feature space.\n",
    "![CART vs Linear Model](assets/cart_vs_linear.png)\n",
    "This happens because at each split made by the tree, only one feature is involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train a classification tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate the classification tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Logistic regression vs classification tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Classification tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section we'll examine how a classification-tree learns from data. Let's start by defining some terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Building Blocks of a Decision-Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A `decision-tree` is a data-structure consisting of a hierarchy of individual units called nodes. A `node` is a point that involves either a question or a prediction. There are three kind of nodes:\n",
    "- The `Root` is the node at which the decision-tree starts growing. It has no parent node and involves a question that gives rise to 2 children nodes through two branches.\n",
    "- The `Internal node` is a node that has a parent. It also involves a question that gives rise to 2 children nodes.\n",
    "- The `Leaf` is a node that has one parent node and involves no questions. It's where a prediction is made.\n",
    "\n",
    "Recall that when a classifictaion tree is trainded on a labeled dataset, the tree learns patterns from the features in such a way to produce the purest leafs. In other words the tree is trained in such a way so that, in each leaf, one class-label is predominant.\n",
    "![Tree Diagram with Notes](assets/tree_diagram_notes.png)\n",
    "In the tree diagram shown above, consider the case where an instrance traverses the tree to reach the leaf on the left. In this leaf, there are 257 instances classified as benign and 7 instances classified as malignant. As a result, the tree's prediction for this instance whould be: `'benign'`.\n",
    "\n",
    "In order to understand how a classification tree produces the purest leafs possible, let's first define the concept of information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Information Gain (IG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The nodes of a classification tree are grown recursively; in other words, the obtention of an internal node or a leaf depends on the state of its predecessors. To produce the purest leafs possible, at each node, a tree asks a question involving one feature `f` and a split-point `sp`. But how dows it know which feature and which split-point to pick? It does so by maximizing information gain!\n",
    "\n",
    "The tree considers that every node contains information and aims at maximizing the Information Gain obtained after each split. Consider the case where a node with N samples is split into a left-node with $N_{left}$ samples and a right-node with $N_{right}$ samples. The information gain for such split is given by the formular shown below:\n",
    "\n",
    "$$ IG(\\underbrace{f}_\\text{feature} , \\underbrace{sp}_\\text{split-point}) = I(parent) - [\\frac{N_{left}}{N} I(left) + \\frac{N_{right}}{N}I(right)] $$\n",
    "\n",
    "A question that you may have in mind here is: \"What criterion is used to measure the impurity of a node?\". Well there are different criteria we can use among which are the `gini-index` and `entropy`.\n",
    "\n",
    "Not what we know what is `Information gain`, let's describe how a classification tree learns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Classification-Tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When an unconstrained tree is trained, the nodes are grown recursively. In other words, a node exists based on the state of its predecessors. At a non-lead node, the data is split based on feature `f` and split-point `sp` in such a way to maximize information gain. If the information gain obtained by splitting a node is null, the node is declared a leaf. \n",
    "\n",
    "Keep in mind that these rules are for unconstrained trees. If you constrain the `maximum depth` of a tree to 2 for example, all nodes having a depth of 2 will be declared leafs even if the information gain obtained by splitting such nodes is not null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Growing a classification tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Using entropy as a criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Entropy vs Gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Decision tree for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this sectio, we'll learn how to train a decision tree for a regression problem. Recall that in regression, the target variable is continous. In other words, the output of the model is a real value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Auto-mpg Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's motivate our discussion of regression by introducing the automobile miles-per-galon dataset from the UCI Machine Learning Repository. This dataset consists of 6 features corresponding to the characteristics of a car and a continous target variable labeled `mpg` which stands for miles-per-galon. Our task is to predict the `mpg` consumption of a car given these six features.\n",
    "\n",
    "To simplify the problem, here the analysis is restricted to only one feature corresponding to the displacement of a car. This feature is denoted by `displ`.\n",
    "\n",
    "A 2D scatter plot of `mpg` versus `displ` shows that the mpg-consumption decreases nonlinearly with displacement. Note that linear models such as `linear regression` would not be able to capture such a non-linear trend.\n",
    "![Non-linear trend](assets/non_linear_trend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Information Criterion for Regression-Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is important to note that, when a regression tree is trained on a dataset, the impurity of a node is measured using the mean-squared error of the targets in that node\n",
    "\n",
    "$$ I(node) = \\underbrace{MSE(node)}_\\text{mean-squared-error} = \\frac{1}{N_{node}} \\sum_{i\\in node} (y^{(i)} - \\hat{y}_{node})^2 $$\n",
    "\n",
    "$$ \\underbrace{\\hat{y}_{node}}_\\text{mean-target-value} = \\frac{1}{N_{node}} \\sum_{i\\in node} y^{(i)}  $$\n",
    "\n",
    "This means that the regression tree tries to find the splits that produce leafs where in each leaf the target values are on average, the closest possible to the mean-value of the labels in that particular leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As a new instance traverses the tree and reaches a certain leaf, its target-variable `'y'` is computed as the average of the target-variables contained in that leaf as shown in the formula below:\n",
    "\n",
    "$$ \\hat{y}_{pred}(leaf) = \\frac{1}{N_{leaf}} \\sum_{i \\in leaf} y^{(i)} $$\n",
    "\n",
    "To highlight the importance of the flexibility of regression trees, take a look at this figure:\n",
    "![Tree vs Linear Regression](assets/tree_vs_linear.png)\n",
    "On the left we have a scatter plot of the data in blue along with the predictions of a linear regression model shown in black. The linear model fails to capture the non-linear trend exhibited by the data. On the right, we have the same scatter plot along with a red line corresponding to the prediction of a regression tree. The regressiont tree shows a greater flexibility and is able to capture the non-linearity, though not fully.\n",
    "\n",
    "In the next chapter, we'll aggregate the predictions of a set of trees that are trained differently to obtain better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train a regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate the regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Linear regression vs regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is one of the fundamental concepts in supervised machine learning. In this chapter, we'll diagnose the problems of overfitting and underfitting. We'll also introduce the concept of ensembling where the predictions of several models are aggregated to produce predictions that are more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning, we make the assumption that there's a mapping `f` between features and labels: `y = f(x)`. The function `f` shown in red below is an unknown function that we want to determine.\n",
    "![f function](assets/f_function.png) \n",
    "In reality, data generation is always accompanied with randomness or noise like the blue points shown here.\n",
    "\n",
    "Our goal is to find a model $ \\hat{f} $ that best approximates $f: \\hat{f} \\approx f $. When training $ \\hat{f} $, we want to make sure that noise is discarded as much as possible. At the end. $\\hat{f}$ should achieve a low predictive error on unseen datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encounter two difficulties when approximating $f$: \n",
    "* The first is `overfitting`, it's when $\\hat{f}(x)$ fits the noise in the training set.\n",
    "* The second is `underfitting`, it's when $\\hat{f}$ is not flexible enough to approximate $f$.\n",
    "\n",
    "**Overfitting**\n",
    "\n",
    "When a model overfits the training set, its predictive power on unseen datasets is pretty low. This is illustrated by the predictions of the decision tree regressor shown below in red:\n",
    "![Overfitting tree](assets/overfitting_tree.png)\n",
    "The model clearly memorized the noise present in the training set. Such model achieves a low training set error and a high test set error.\n",
    "\n",
    "**Underfitting**\n",
    "\n",
    "When a model underfits the data, like regression tree whose predictions are shown below in red, the training set error is roughly equal to the test set error:\n",
    "![Underfitting tree](assets/underfitting_tree.png)\n",
    "However, both errors are relatively high. Now the trained model isn't flexible enough to capture the complex dependency between features and labels. In analogy, it's like teaching calculus to a 3-year old. The child does not ave the required mental abstraction level that enables him to understand calculus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generalization error` of a model tells us how much it generalizes on unseen data. It can be decomposed into 3 terms: bias, variance and irreducible error where the irreducible error is the error contribution of noise.\n",
    "$$ \\hat{f} = bias^2 + variance + irreducible\\_error $$\n",
    "\n",
    "The `Bias` term tells us, on average, how much $ \\hat{f} \\neq f $. To illustrate this consider the high bias model shown here in black:\n",
    "![Tree Bias](assets/tree_bias.png)\n",
    "this model is not flexible enough to approximate the true function $f$ shown in red. High bias models lead to underfitting.\n",
    "\n",
    "The `Variance` term tells us how much $\\hat{f}$ is inconsistent over different training sets. Conside the high variance model shown below in black:\n",
    "![Tree Variance](assets/tree_variance.png)\n",
    "in this case, $\\hat{f}$ follows the training data points so closely that is misses the true function $f$ shwon in red. High variance models lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of a model sets its flexiblity to approximate the true function $f$. For example: increasing the `maximum-tree-depth` increases the complexity of a decision tree. The diagram below shows how the best model complexity corresponds to the lowest generalization error:\n",
    "![Tree trade off](assets/tree_off.png)\n",
    "When the model complexity increases, the variance increases while the bias decreases. Conversely, when model complexity decreases, variance decreases and bias increases. Our goal is to find the model complexity that achieves the lowest generalization error. Since this error is the sum of three terms with the irreducible error being constant, we need to find a balance between bias and variance because as one increases the other decreases. This is known as the bias-variance trade-off.\n",
    "\n",
    "Visually, we can imagine approximating $\\hat{f}$ as aiming at the center of a shooting-target where the center is the true function $f$:\n",
    "![Bias-Variance Tradeoff](assets/target_practice.png)\n",
    "If $\\hat{f}$ is low bias and low variance, our shots will be closely clustered around the center. If $\\hat{f}$ is high variance and high bias, not only will our shots miss the target but they would also be spread all around the shooting target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity, biad and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and under fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose bias and variance problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the training error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High bias or high variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate individual classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better performance with a Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bagging and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Bagging is an ensemble method involving training the same algorithm many times using different subsets sampled from the training data. In this chapter, we'll show how bagging can be used to create a tree ensemble. We'll also show how the random forests algorithm can lead to further ensemble diversity through randomization at the level of each split in the trees forming the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Define the bagging classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate Bagging performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Out of Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prepare the ground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### OOB Score vs Test Set Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Random Forests (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train a RF regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate the RF regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualizing features importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Boosting refers to an ensemble method in which several models are trained sequentially with each model learning from the errors of its predecessors. In this chapter, we'll introduce two boosting methods: AdaBoost and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Define the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Gradient Boosting (GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Define the GB regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train the GB regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate the GB regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Stochastic Gradient Boosting (SGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Regression with SGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train the SGB regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate the SGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
