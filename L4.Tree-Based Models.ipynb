{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ML-Challenge/week3-supervised-learning/blob/master/L4.Tree-Based%20Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are supervised learning models used for problems involving classification and regression. Tree models present a high flexibility that comes at a price: on one hand, trees are able to capture complex non-linear relationships; on the other hand, they are prone to memorizing the noise present in a dataset. By aggregating the predictions of trees that are trained differently, ensemble methods take advantage of the flexibility of trees while reducing their tendency to memorize noise. Ensemble methods are used across a variety of fields and have a proven track record of winning many machine learning competitions. In this lesson, we'll learn how to use Python to train decision trees and tree-based models with the user-friendly scikit-learn machine learning library. We'll see the advantages and shortcomings of trees and demonstrate how ensembling can alleviate these shortcomings, all while practicing on real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:18.466964Z",
     "start_time": "2020-02-11T09:27:18.464524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download utils.py to working directory\n",
    "#import urllib.request\n",
    "#urllib.request.urlretrieve('https://raw.githubusercontent.com/ML-Challenge/week3-supervised-learning/master/utils.py', 'utils.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:19.370268Z",
     "start_time": "2020-02-11T09:27:18.469160Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import utils\n",
    "# We'll be using this module throughout the lesson\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:19.374889Z",
     "start_time": "2020-02-11T09:27:19.371772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# and setting the size of all plots.\n",
    "plt.rcParams['figure.figsize'] = [11, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Regression Trees (CART) are a set of supervised learning models used for problems involving classification and regression. In this chapter, we'll introduce the CART algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification-tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a labeled dataset, a classification tree learns a sequence of if-else questions about individual features in order to infer the labels. In contrast to linear models, trees are able to capture non-linear relantionships between features and labels. In addition, trees don't require the features to be on the same scale through standardization for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand trees more concretely, we'll try to predict whether a tumor is malignant or benign in the Wisconsing Breast Cancer dataset using only 2 features.\n",
    "\n",
    "![Breast Cancer](assets/breast_cancer.png)\n",
    "\n",
    "The figure here shows a scatterplot of two cancerous cell features with malignant-tumors in blue and benign-tumors in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision-tree Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a classification tree is trained on this dataset, the tree learns a sequence of if-else questions with each question involving one feature and one split-point. Take a look at the tree diagram:\n",
    "\n",
    "![Tree Diagram](assets/tree_diagram.png)\n",
    "\n",
    "At the top, the tree asks wheter the concave-points mean of an instance is smaller than 0.051. It it is, the instance traverses the `True` branch; otherwise, it traverses the `False` branch. Similarly, the instance keeps traversing the internal branches until it reaches an end. The label of the instance is then predicted to be that of the prevailing class at that end.\n",
    "\n",
    "The maximum number of branches separating the top from an extreme-end is known as the `maximum depth` which is equal to 2 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification-model divides the feature-space into regions where all instances in one region are assigned to only one class-label. These regions are known as decision-regions. Decision-regions are separated by surfaces called decision-boundaries.\n",
    "\n",
    "![Linear classifier decision region](assets/linear_decision.png)\n",
    "\n",
    "The figure above shows the decision-regions of a linear-classifier. Not how the boundary is a straight-line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Regions: CART vs. Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, as shown here on the right, a classification-tree produces rectangular decision-regions in the feature space.\n",
    "\n",
    "![CART vs Linear Model](assets/cart_vs_linear.png)\n",
    "\n",
    "This happens because at each split made by the tree, only one feature is involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a classification tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll work with the [Wisconsin Breast Cancer Dataset](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data) from the UCI machine learning repository. We'll predict whether a tumor is malignant or benign based on two features: the mean radius of the tumor (`radius_mean`) and its mean number of concave points (`concave points_mean`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.121302Z",
     "start_time": "2020-02-11T09:27:20.099133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.wbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.127392Z",
     "start_time": "2020-02-11T09:27:20.123031Z"
    }
   },
   "outputs": [],
   "source": [
    "X = utils.wbc[['radius_mean', 'concave points_mean']]\n",
    "y = utils.wbc['diagnosis'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.134798Z",
     "start_time": "2020-02-11T09:27:20.129466Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = utils.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already loaded and is split into 80% train and 20% test. The feature matrices are assigned to `X_train` and `X_test`, while the arrays of labels are assigned to `y_train` and `y_test` where class 1 corresponds to a malignant tumor and class 0 corresponds to a benign tumor. To obtain reproducible results, we also defined a variable called `SEED` which is set to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.194041Z",
     "start_time": "2020-02-11T09:27:20.191707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.200220Z",
     "start_time": "2020-02-11T09:27:20.197938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.207929Z",
     "start_time": "2020-02-11T09:27:20.201855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.213897Z",
     "start_time": "2020-02-11T09:27:20.209416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the classification tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fit our first classification tree, it's time to evaluate its performance on the test set. We'll do so using the accuracy metric which corresponds to the fraction of correct predictions made on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.379338Z",
     "start_time": "2020-02-11T09:27:20.376589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.387498Z",
     "start_time": "2020-02-11T09:27:20.383839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test set accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Using only two features, our tree was able to achieve an accuracy of 89%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression vs classification tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will compare logistic regression and the classification tree on trained on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.721393Z",
     "start_time": "2020-02-11T09:27:20.719009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import LogisticRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.729142Z",
     "start_time": "2020-02-11T09:27:20.726286Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instatiate logreg\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.739086Z",
     "start_time": "2020-02-11T09:27:20.731139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=1, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit logreg to the training set\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.746413Z",
     "start_time": "2020-02-11T09:27:20.741262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict test set labels\n",
    "logreg_pred = logreg.predict(X_test)\n",
    "print(logreg_pred[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:20.752526Z",
     "start_time": "2020-02-11T09:27:20.747979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Compute test set accuracy  \n",
    "logreg_acc = accuracy_score(y_test, logreg_pred)\n",
    "print(f\"Test set accuracy: {logreg_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll examine how a classification-tree learns from data. Let's start by defining some terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Blocks of a Decision-Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `decision-tree` is a data-structure consisting of a hierarchy of individual units called nodes. A `node` is a point that involves either a question or a prediction. There are three kind of nodes:\n",
    "- The `Root` is the node at which the decision-tree starts growing. It has no parent node and involves a question that gives rise to 2 children nodes through two branches.\n",
    "- The `Internal node` is a node that has a parent. It also involves a question that gives rise to 2 children nodes.\n",
    "- The `Leaf` is a node that has one parent node and involves no questions. It's where a prediction is made.\n",
    "\n",
    "Recall that when a classifictaion tree is trainded on a labeled dataset, the tree learns patterns from the features in such a way to produce the purest leafs. In other words the tree is trained in such a way so that, in each leaf, one class-label is predominant.\n",
    "\n",
    "![Tree Diagram with Notes](assets/tree_diagram_notes.png)\n",
    "\n",
    "In the tree diagram shown above, consider the case where an instrance traverses the tree to reach the leaf on the left. In this leaf, there are 257 instances classified as benign and 7 instances classified as malignant. As a result, the tree's prediction for this instance whould be: `'benign'`.\n",
    "\n",
    "In order to understand how a classification tree produces the purest leafs possible, let's first define the concept of information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain (IG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes of a classification tree are grown recursively; in other words, the obtention of an internal node or a leaf depends on the state of its predecessors. To produce the purest leafs possible, at each node, a tree asks a question involving one feature `f` and a split-point `sp`. But how dows it know which feature and which split-point to pick? It does so by maximizing information gain!\n",
    "\n",
    "The tree considers that every node contains information and aims at maximizing the Information Gain obtained after each split. Consider the case where a node with N samples is split into a left-node with $N_{left}$ samples and a right-node with $N_{right}$ samples. The information gain for such split is given by the formular shown below:\n",
    "\n",
    "$$ IG(\\underbrace{f}_\\text{feature} , \\underbrace{sp}_\\text{split-point}) = I(parent) - [\\frac{N_{left}}{N} I(left) + \\frac{N_{right}}{N}I(right)] $$\n",
    "\n",
    "A question that you may have in mind here is: \"What criterion is used to measure the impurity of a node?\". Well there are different criteria we can use among which are the `gini-index` and `entropy`.\n",
    "\n",
    "Not what we know what is `Information gain`, let's describe how a classification tree learns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification-Tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an unconstrained tree is trained, the nodes are grown recursively. In other words, a node exists based on the state of its predecessors. At a non-lead node, the data is split based on feature `f` and split-point `sp` in such a way to maximize information gain. If the information gain obtained by splitting a node is null, the node is declared a leaf. \n",
    "\n",
    "Keep in mind that these rules are for unconstrained trees. If you constrain the `maximum depth` of a tree to 2 for example, all nodes having a depth of 2 will be declared leafs even if the information gain obtained by splitting such nodes is not null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using entropy as a criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll train a classification tree on the Wisconsin Breast Cancer dataset using entropy as an information criterion. We'll do so using all the 30 features in the dataset, which we split into 80% train and 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:21.698635Z",
     "start_time": "2020-02-11T09:27:21.692542Z"
    }
   },
   "outputs": [],
   "source": [
    "X = utils.wbc.drop('diagnosis', axis=1)\n",
    "y = utils.wbc['diagnosis'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:21.707895Z",
     "start_time": "2020-02-11T09:27:21.700223Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:21.715296Z",
     "start_time": "2020-02-11T09:27:21.711350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:21.731518Z",
     "start_time": "2020-02-11T09:27:21.717315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=8,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy vs Gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll compare the test set accuracy of `dt_entropy` to the accuracy of another tree named `dt_gini`. The tree `dt_gini` was trained on the same dataset using the same parameters except for the information criterion which was set to the gini index using the keyword `'gini'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:21.932156Z",
     "start_time": "2020-02-11T09:27:21.918401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate dt_gini, set 'gini' as the information criterion\n",
    "dt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=utils.SEED)\n",
    "\n",
    "# Fit dt_gini to the training set\n",
    "dt_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:21.940268Z",
     "start_time": "2020-02-11T09:27:21.933776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use dt_gini to predict test set labels\n",
    "y_pred_gini= dt_gini.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:21.946171Z",
     "start_time": "2020-02-11T09:27:21.942391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using the gini index:  0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy_gini\n",
    "print('Accuracy achieved by using the gini index: ', accuracy_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:21.952305Z",
     "start_time": "2020-02-11T09:27:21.947602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use dt_entropy to predict test set labels\n",
    "y_pred_entropy= dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:21.957990Z",
     "start_time": "2020-02-11T09:27:21.954817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy:  0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy_entropy\n",
    "print('Accuracy achieved by using entropy: ', accuracy_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sectio, we'll learn how to train a decision tree for a regression problem. Recall that in regression, the target variable is continous. In other words, the output of the model is a real value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-mpg Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's motivate our discussion of regression by introducing the automobile miles-per-galon dataset from the UCI Machine Learning Repository. This dataset consists of 6 features corresponding to the characteristics of a car and a continous target variable labeled `mpg` which stands for miles-per-galon. Our task is to predict the `mpg` consumption of a car given these six features.\n",
    "\n",
    "To simplify the problem, here the analysis is restricted to only one feature corresponding to the displacement of a car. This feature is denoted by `displ`.\n",
    "\n",
    "A 2D scatter plot of `mpg` versus `displ` shows that the mpg-consumption decreases nonlinearly with displacement. Note that linear models such as `linear regression` would not be able to capture such a non-linear trend.\n",
    "\n",
    "![Non-linear trend](assets/non_linear_trend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Criterion for Regression-Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that, when a regression tree is trained on a dataset, the impurity of a node is measured using the mean-squared error of the targets in that node\n",
    "\n",
    "$$ I(node) = \\underbrace{MSE(node)}_\\text{mean-squared-error} = \\frac{1}{N_{node}} \\sum_{i\\in node} (y^{(i)} - \\hat{y}_{node})^2 $$\n",
    "\n",
    "$$ \\underbrace{\\hat{y}_{node}}_\\text{mean-target-value} = \\frac{1}{N_{node}} \\sum_{i\\in node} y^{(i)}  $$\n",
    "\n",
    "This means that the regression tree tries to find the splits that produce leafs where in each leaf the target values are on average, the closest possible to the mean-value of the labels in that particular leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a new instance traverses the tree and reaches a certain leaf, its target-variable `'y'` is computed as the average of the target-variables contained in that leaf as shown in the formula below:\n",
    "\n",
    "$$ \\hat{y}_{pred}(leaf) = \\frac{1}{N_{leaf}} \\sum_{i \\in leaf} y^{(i)} $$\n",
    "\n",
    "To highlight the importance of the flexibility of regression trees, take a look at this figure:\n",
    "\n",
    "![Tree vs Linear Regression](assets/tree_vs_linear.png)\n",
    "\n",
    "On the left we have a scatter plot of the data in blue along with the predictions of a linear regression model shown in black. The linear model fails to capture the non-linear trend exhibited by the data. On the right, we have the same scatter plot along with a red line corresponding to the prediction of a regression tree. The regressiont tree shows a greater flexibility and is able to capture the non-linearity, though not fully.\n",
    "\n",
    "In the next chapter, we'll aggregate the predictions of a set of trees that are trained differently to obtain better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll train a regression tree to predict the `mpg` (miles per gallon) consumption of cars in the [auto-mpg dataset](https://www.kaggle.com/uciml/autompg-dataset) using all the six available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.119727Z",
     "start_time": "2020-02-11T09:27:23.108527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>displ</th>\n",
       "      <th>hp</th>\n",
       "      <th>weight</th>\n",
       "      <th>accel</th>\n",
       "      <th>size</th>\n",
       "      <th>origin_Asia</th>\n",
       "      <th>origin_Europe</th>\n",
       "      <th>origin_US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3139</td>\n",
       "      <td>14.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>193</td>\n",
       "      <td>4732</td>\n",
       "      <td>18.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>60</td>\n",
       "      <td>1800</td>\n",
       "      <td>16.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>98</td>\n",
       "      <td>3525</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2188</td>\n",
       "      <td>15.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  displ   hp  weight  accel  size  origin_Asia  origin_Europe  \\\n",
       "0  18.0  250.0   88    3139   14.5  15.0            0              0   \n",
       "1   9.0  304.0  193    4732   18.5  20.0            0              0   \n",
       "2  36.1   91.0   60    1800   16.4  10.0            1              0   \n",
       "3  18.5  250.0   98    3525   19.0  15.0            0              0   \n",
       "4  34.3   97.0   78    2188   15.8  10.0            0              1   \n",
       "\n",
       "   origin_US  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.125639Z",
     "start_time": "2020-02-11T09:27:23.121665Z"
    }
   },
   "outputs": [],
   "source": [
    "X = utils.auto.drop('mpg', axis=1)\n",
    "y = utils.auto['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.133277Z",
     "start_time": "2020-02-11T09:27:23.127960Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.137211Z",
     "start_time": "2020-02-11T09:27:23.134832Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.140818Z",
     "start_time": "2020-02-11T09:27:23.138572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.147543Z",
     "start_time": "2020-02-11T09:27:23.142170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=8, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=0.13,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will evaluate the test set performance of `dt` using the Root Mean Squared Error (RMSE) metric. The RMSE of a model measures, on average, how much the model's predictions differ from the actual labels. The RMSE of a model can be obtained by computing the square root of the model's Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.323809Z",
     "start_time": "2020-02-11T09:27:23.320565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.330582Z",
     "start_time": "2020-02-11T09:27:23.327539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.335284Z",
     "start_time": "2020-02-11T09:27:23.332233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.338997Z",
     "start_time": "2020-02-11T09:27:23.336762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.343882Z",
     "start_time": "2020-02-11T09:27:23.341131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 4.27\n"
     ]
    }
   ],
   "source": [
    "# Print rmse_dt\n",
    "print(f\"Test set RMSE of dt: {rmse_dt:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression vs regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you'll compare the test set RMSE of `dt` to that achieved by a linear regression model. We have already instantiated a linear regression model lr and trained it on the same dataset as dt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.535877Z",
     "start_time": "2020-02-11T09:27:23.533415Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.552208Z",
     "start_time": "2020-02-11T09:27:23.539954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.558132Z",
     "start_time": "2020-02-11T09:27:23.554102Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_lr = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.563903Z",
     "start_time": "2020-02-11T09:27:23.560603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute mse_lr\n",
    "mse_lr = MSE(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.568380Z",
     "start_time": "2020-02-11T09:27:23.566004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute rmse_lr\n",
    "rmse_lr = mse_lr ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.573671Z",
     "start_time": "2020-02-11T09:27:23.570114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression test set RMSE: 3.98\n"
     ]
    }
   ],
   "source": [
    "# Print rmse_lr\n",
    "print(f'Linear Regression test set RMSE: {rmse_lr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:23.578153Z",
     "start_time": "2020-02-11T09:27:23.575482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Tree test set RMSE: 4.27\n"
     ]
    }
   ],
   "source": [
    "# Print rmse_dt\n",
    "print(f'Regression Tree test set RMSE: {rmse_dt:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is one of the fundamental concepts in supervised machine learning. In this chapter, we'll diagnose the problems of overfitting and underfitting. We'll also introduce the concept of ensembling where the predictions of several models are aggregated to produce predictions that are more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning, we make the assumption that there's a mapping `f` between features and labels: `y = f(x)`. The function `f` shown in red below is an unknown function that we want to determine.\n",
    "\n",
    "![f function](assets/f_function.png) \n",
    "\n",
    "In reality, data generation is always accompanied with randomness or noise like the blue points shown here.\n",
    "\n",
    "Our goal is to find a model $ \\hat{f} $ that best approximates $f: \\hat{f} \\approx f $. When training $ \\hat{f} $, we want to make sure that noise is discarded as much as possible. At the end. $\\hat{f}$ should achieve a low predictive error on unseen datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encounter two difficulties when approximating $f$: \n",
    "* The first is `overfitting`, it's when $\\hat{f}(x)$ fits the noise in the training set.\n",
    "* The second is `underfitting`, it's when $\\hat{f}$ is not flexible enough to approximate $f$.\n",
    "\n",
    "**Overfitting**\n",
    "\n",
    "When a model overfits the training set, its predictive power on unseen datasets is pretty low. This is illustrated by the predictions of the decision tree regressor shown below in red:\n",
    "\n",
    "![Overfitting tree](assets/overfitting_tree.png)\n",
    "\n",
    "The model clearly memorized the noise present in the training set. Such model achieves a low training set error and a high test set error.\n",
    "\n",
    "**Underfitting**\n",
    "\n",
    "When a model underfits the data, like regression tree whose predictions are shown below in red, the training set error is roughly equal to the test set error:\n",
    "\n",
    "![Underfitting tree](assets/underfitting_tree.png)\n",
    "\n",
    "However, both errors are relatively high. Now the trained model isn't flexible enough to capture the complex dependency between features and labels. In analogy, it's like teaching calculus to a 3-year old. The child does not ave the required mental abstraction level that enables him to understand calculus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generalization error` of a model tells us how much it generalizes on unseen data. It can be decomposed into 3 terms: bias, variance and irreducible error where the irreducible error is the error contribution of noise.\n",
    "$$ \\hat{f} = bias^2 + variance + irreducible\\_error $$\n",
    "\n",
    "The `Bias` term tells us, on average, how much $ \\hat{f} \\neq f $. To illustrate this consider the high bias model shown here in black:\n",
    "![Tree Bias](assets/tree_bias.png)\n",
    "this model is not flexible enough to approximate the true function $f$ shown in red. High bias models lead to underfitting.\n",
    "\n",
    "The `Variance` term tells us how much $\\hat{f}$ is inconsistent over different training sets. Conside the high variance model shown below in black:\n",
    "![Tree Variance](assets/tree_variance.png)\n",
    "in this case, $\\hat{f}$ follows the training data points so closely that is misses the true function $f$ shwon in red. High variance models lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of a model sets its flexiblity to approximate the true function $f$. For example: increasing the `maximum-tree-depth` increases the complexity of a decision tree. The diagram below shows how the best model complexity corresponds to the lowest generalization error:\n",
    "\n",
    "![Tree trade off](assets/tree_off.png)\n",
    "\n",
    "When the model complexity increases, the variance increases while the bias decreases. Conversely, when model complexity decreases, variance decreases and bias increases. Our goal is to find the model complexity that achieves the lowest generalization error. Since this error is the sum of three terms with the irreducible error being constant, we need to find a balance between bias and variance because as one increases the other decreases. This is known as the bias-variance trade-off.\n",
    "\n",
    "Visually, we can imagine approximating $\\hat{f}$ as aiming at the center of a shooting-target where the center is the true function $f$:\n",
    "\n",
    "![Bias-Variance Tradeoff](assets/target_practice.png)\n",
    "\n",
    "If $\\hat{f}$ is low bias and low variance, our shots will be closely clustered around the center. If $\\hat{f}$ is high variance and high bias, not only will our shots miss the target but they would also be spread all around the shooting target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose bias and variance problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we've trained a supervised machine learning model labeled $\\hat{f}$, how do we estimate the $\\hat{f}$'s generalization error? This cannot be done directly because:\n",
    "\n",
    "* $f$ is unknown\n",
    "* usually we only have one dataset\n",
    "* we don't have access to the error term due to noise.\n",
    "\n",
    "A solution to this is to first split the data into a training and test set. The model $\\hat{f}$ can be then fit to the training set and its error can be evaluated on the test set. The generalization error of $\\hat{f}$ is roughly approxiamted by $\\hat{f}$ error on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Model Evaluation with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the test set should be kept untouched until one is confident about $\\hat{f}$'s performance. It should only be used to evaluate $\\hat{f}$'s final performance or error. Now, evaluating $\\hat{f}$'s performance on the training set may produce an optimistic estimation of the error because $\\hat{f}$ was already exposed to the training set when it was fit.\n",
    "\n",
    "To obtain a reliable estimate of $\\hat{f}$'s performance, we use a technique called cross-validation or CV. CV can be performed using:\n",
    "\n",
    "* K-Fold CV\n",
    "* Hold-out CV\n",
    "\n",
    "In this chapter we are going over the K-fold CV. The diagram below illustrates this technique for K=10:\n",
    "\n",
    "![10 K-fold CV](assets/k_fold_10.png)\n",
    "\n",
    "* First the training set (T) is split randomly into 10 partitions or folds\n",
    "* The error of $\\hat{f}$ is evaluated 10 times on the 10 folds\n",
    "* Each time, one fold is picked for evaluation after training $\\hat{f}$ on the other 9 folds\n",
    "* At the end, we obtain a list of 10 errors\n",
    "\n",
    "Finally, as shown in this formula, the CV-error is computed as the mean of the 10 obtained errors:\n",
    "$$ CV_{error} = \\frac{\\sum_{i=1}^n E_i}{n} $$\n",
    "\n",
    "Once we have computed $\\hat{f}$'s cross-validation-error, we can check if it is greater than $\\hat{f}$'s training set error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Diagnose Variance Problems**\n",
    "\n",
    "If $\\hat{f}$ suffers from `high variance`: CV error of $\\hat{f}$ > training set error of $\\hat{f}$. In such case $\\hat{f}$ has overfit the training set. To remedy this we can try the following:\n",
    "\n",
    "* decrease model complexity. For example, in a decision tree we can reduce the `maximum-tree-depth` or increase the `maximum-samples-per-leaf`.\n",
    "* gather more data to train $\\hat{f}$\n",
    "\n",
    "**Diagnose Bias Problems**\n",
    "\n",
    "On the other hand, $\\hat{f}$ is said to suffer from high bias if: CV error of $\\hat{f} \\approx$ training set error of $\\hat{f} >>$ desired error. In such case $\\hat{f}$ underfits the training set. To remedy this we can try the following:\n",
    "\n",
    "* increase model complexity. For example, in a decision tree increase max depth or decrease min samples per leaf\n",
    "* gather more relevant features for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following set of examples, we'll diagnose the bias and variance problems of a regression tree. The regression tree we'll define in this exercise will be used to predict the mpg consumption of cars from the auto dataset using all available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:25.432415Z",
     "start_time": "2020-02-11T09:27:25.428487Z"
    }
   },
   "outputs": [],
   "source": [
    "X = utils.auto.drop('mpg', axis=1)\n",
    "y = utils.auto['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:25.438306Z",
     "start_time": "2020-02-11T09:27:25.434010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:25.446176Z",
     "start_time": "2020-02-11T09:27:25.443776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the 10-fold CV error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll evaluate the 10-fold CV Root Mean Squared Error (RMSE) achieved by the regression tree `dt` that we instantiated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:25.638737Z",
     "start_time": "2020-02-11T09:27:25.636520Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.659181Z",
     "start_time": "2020-02-11T09:27:25.642328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.666234Z",
     "start_time": "2020-02-11T09:27:32.662404Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.672814Z",
     "start_time": "2020-02-11T09:27:32.669022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 5.14\n"
     ]
    }
   ],
   "source": [
    "# Print RMSE_CV\n",
    "print(f'CV RMSE: {RMSE_CV:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very good practice is to keep the test set untouched until we are confident about your model's performance. CV is a great technique to get an estimate of a model's performance without affecting the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the training error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now evaluate the training set RMSE achieved by the regression tree dt that you instantiated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.684877Z",
     "start_time": "2020-02-11T09:27:32.675401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=0.26,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.690409Z",
     "start_time": "2020-02-11T09:27:32.686539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.696375Z",
     "start_time": "2020-02-11T09:27:32.692652Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.701380Z",
     "start_time": "2020-02-11T09:27:32.698189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 5.15\n"
     ]
    }
   ],
   "source": [
    "# Print RMSE_train\n",
    "print(f'Train RMSE: {RMSE_train:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we will look at a supervised learning technique known as ensemble learning. Let's first recap what we learned from the previous chapter about CARTs.\n",
    "\n",
    "**Advantages of CARTs**\n",
    "\n",
    "CARTs present many advantages:\n",
    "\n",
    "* For example they are easy to understand and their output is easy to interpret. \n",
    "* In addition, CARTs are easy to use and their flexibility gives them an ability to describe nonlinear dependencies between features and labels. \n",
    "* Moreover, we don't need a lot of feature preprocessing to train a CART. \n",
    "* In contrast to other models, we don't have to standardize or normalize features before feeding them to a CART.\n",
    "\n",
    "**Limitations of CARTs**\n",
    "\n",
    "CARTs also have limitations:\n",
    "\n",
    "* A classification tree for example, is only able to produce orthogonal decision boundaries.\n",
    "* CARTs are also very sensitive to small variations in the training set. Sometimes, when a single point is removed from the training set, a CARTs learned parameters may change drastically.\n",
    "* CARTs also suffer from high variance when they are trained without constraints. In such case, they may overfit the training set.\n",
    "\n",
    "A solution that takes advantage of the flexibility of CARTs while reducing their tendency to memorize noise is ensemble learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble learning can be summarized as follows:\n",
    "\n",
    "* Train different models on the same dataset.\n",
    "* Let each model make its predictions.\n",
    "* A meta-model aggregates predictions of individual models and outputs a final prediction\n",
    "\n",
    "The final prediction is more robust and less prone to errors than each individual model. The best results are obtained when the models are skillful but in different way meaning that if some models make predictions that are way off, the other models should compensate these errors. In such case, the meat-model's predictions are more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the diagram below to visually understand how ensemble learning works for a classification problem:\n",
    "\n",
    "![Ensemble Learning](assets/ensemble.png)\n",
    "\n",
    "* First the training set is fed to different classifiers\n",
    "* Each classifier learns its parameters and makes predictions\n",
    "* The predictions are fed to a meta model which aggregates them and outputs a final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at an ensemble technique known as the voting classifier. More concretely, we'll consider a binary classification task. The ensemble here consists of N classifiers making the predictions $ P_0, P_1, ... , P_N $ with $P_i$ = 0 or 1. The meta-model outputs the final prediction by hard voting.\n",
    "\n",
    "To understand hard voting, consider a voting classifier that consists of 3 trained classifiers as shown in the diagram below:\n",
    "\n",
    "![Voting classifier](assets/voting_classifier.png)\n",
    "\n",
    "While classifiers 1 and 3 predict the label of 1 for a new data-point, classifier 2 predicts the label 0. In this case, 1 has 2 votes while 0 has 1 vote. As a result, the voting classifier predicts 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following set of examples, we'll work with the [Indian Liver Patient Dataset](https://www.kaggle.com/jeevannagaraj/indian-liver-patient-dataset) from the UCI Machine learning repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll instantiate three classifiers to predict whether a patient suffers from a liver disease using all the features present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.718171Z",
     "start_time": "2020-02-11T09:27:32.704092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_std</th>\n",
       "      <th>Total_Bilirubin_std</th>\n",
       "      <th>Direct_Bilirubin_std</th>\n",
       "      <th>Alkaline_Phosphotase_std</th>\n",
       "      <th>Alamine_Aminotransferase_std</th>\n",
       "      <th>Aspartate_Aminotransferase_std</th>\n",
       "      <th>Total_Protiens_std</th>\n",
       "      <th>Albumin_std</th>\n",
       "      <th>Albumin_and_Globulin_Ratio_std</th>\n",
       "      <th>Is_male_std</th>\n",
       "      <th>Liver_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.247403</td>\n",
       "      <td>-0.420320</td>\n",
       "      <td>-0.495414</td>\n",
       "      <td>-0.428870</td>\n",
       "      <td>-0.355832</td>\n",
       "      <td>-0.319111</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.147390</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.062306</td>\n",
       "      <td>1.218936</td>\n",
       "      <td>1.423518</td>\n",
       "      <td>1.675083</td>\n",
       "      <td>-0.093573</td>\n",
       "      <td>-0.035962</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.077462</td>\n",
       "      <td>-0.648461</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.062306</td>\n",
       "      <td>0.640375</td>\n",
       "      <td>0.926017</td>\n",
       "      <td>0.816243</td>\n",
       "      <td>-0.115428</td>\n",
       "      <td>-0.146459</td>\n",
       "      <td>0.478274</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.178707</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.815511</td>\n",
       "      <td>-0.372106</td>\n",
       "      <td>-0.388807</td>\n",
       "      <td>-0.449416</td>\n",
       "      <td>-0.366760</td>\n",
       "      <td>-0.312205</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.165780</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.679294</td>\n",
       "      <td>0.093956</td>\n",
       "      <td>0.179766</td>\n",
       "      <td>-0.395996</td>\n",
       "      <td>-0.295731</td>\n",
       "      <td>-0.177537</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>-0.930414</td>\n",
       "      <td>-1.713237</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age_std  Total_Bilirubin_std  Direct_Bilirubin_std  \\\n",
       "0  1.247403            -0.420320             -0.495414   \n",
       "1  1.062306             1.218936              1.423518   \n",
       "2  1.062306             0.640375              0.926017   \n",
       "3  0.815511            -0.372106             -0.388807   \n",
       "4  1.679294             0.093956              0.179766   \n",
       "\n",
       "   Alkaline_Phosphotase_std  Alamine_Aminotransferase_std  \\\n",
       "0                 -0.428870                     -0.355832   \n",
       "1                  1.675083                     -0.093573   \n",
       "2                  0.816243                     -0.115428   \n",
       "3                 -0.449416                     -0.366760   \n",
       "4                 -0.395996                     -0.295731   \n",
       "\n",
       "   Aspartate_Aminotransferase_std  Total_Protiens_std  Albumin_std  \\\n",
       "0                       -0.319111            0.293722     0.203446   \n",
       "1                       -0.035962            0.939655     0.077462   \n",
       "2                       -0.146459            0.478274     0.203446   \n",
       "3                       -0.312205            0.293722     0.329431   \n",
       "4                       -0.177537            0.755102    -0.930414   \n",
       "\n",
       "   Albumin_and_Globulin_Ratio_std  Is_male_std  Liver_disease  \n",
       "0                       -0.147390            0              1  \n",
       "1                       -0.648461            1              1  \n",
       "2                       -0.178707            1              1  \n",
       "3                        0.165780            1              1  \n",
       "4                       -1.713237            1              1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.liver_patients.head()                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.723315Z",
     "start_time": "2020-02-11T09:27:32.720460Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the classes used for the enesemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.727722Z",
     "start_time": "2020-02-11T09:27:32.725124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate lr\n",
    "lr = LogisticRegression(solver='liblinear' ,random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.731376Z",
     "start_time": "2020-02-11T09:27:32.729087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate knn\n",
    "knn = KNN(n_neighbors=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.735467Z",
     "start_time": "2020-02-11T09:27:32.733099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.740260Z",
     "start_time": "2020-02-11T09:27:32.737490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate individual classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll evaluate the performance of the models in the list classifiers that we defined in the previous exercise. We'll do so by fitting each classifier on the training set and evaluating its test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.745364Z",
     "start_time": "2020-02-11T09:27:32.741655Z"
    }
   },
   "outputs": [],
   "source": [
    "X = utils.liver_patients.drop('Liver_disease', axis=1)\n",
    "y = utils.liver_patients['Liver_disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.752106Z",
     "start_time": "2020-02-11T09:27:32.746968Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.773003Z",
     "start_time": "2020-02-11T09:27:32.753579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.764\n",
      "K Nearest Neighbours : 0.701\n",
      "Classification Tree : 0.730\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "    \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred) \n",
    "    \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print(f'{clf_name:s} : {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `LogisticRegression' got the highest accuracy of 76.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better performance with a Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll evaluate the performance of a voting classifier that takes the outputs of the models defined in the list classifiers and assigns labels by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.776884Z",
     "start_time": "2020-02-11T09:27:32.774411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import VotingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.781009Z",
     "start_time": "2020-02-11T09:27:32.778298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a VotingClassifier vc\n",
    "vc = VotingClassifier(estimators=classifiers)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.797153Z",
     "start_time": "2020-02-11T09:27:32.782535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('Logistic Regression',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='warn',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=1,\n",
       "                                                 solver='liblinear', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('K Nearest Neighbours',\n",
       "                              KNeighborsClassifier(algorithm='auto',\n",
       "                                                   l...\n",
       "                             ('Classification Tree',\n",
       "                              DecisionTreeClassifier(class_weight=None,\n",
       "                                                     criterion='gini',\n",
       "                                                     max_depth=None,\n",
       "                                                     max_features=None,\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=0.13,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     presort=False,\n",
       "                                                     random_state=1,\n",
       "                                                     splitter='best'))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.811044Z",
     "start_time": "2020-02-11T09:27:32.798603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.816240Z",
     "start_time": "2020-02-11T09:27:32.813015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.770\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Voting Classifier: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the voting classifier achieves a test set accuracy of 77%. This value is greater than that achieved by LogisticRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is an ensemble method involving training the same algorithm many times using different subsets sampled from the training data. In this chapter, we'll show how bagging can be used to create a tree ensemble. We'll also show how the random forests algorithm can lead to further ensemble diversity through randomization at the level of each split in the trees forming the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we'll introduce an ensemble method known as Bootstrap aggregation or Bagging. In the last chapter we learned that the Voting Classifier is an ensemble of models that are fit to the same training set using different algorithms. We also say that the final predictions were obtained by majority voting. \n",
    "\n",
    "In `Bagging`, the ensemble is formed by models that use the same training algorithm. However, these models are not trained on the entire training set. Instead, each model is trained on a different subset of the data. In fact, Bagging stands for Bootstrap Aggregation.Its name refers to the fact that it uses a technique known as the `bootstrap`. Overall, Bagging has the effect of reducing the variance of individual models in the ensemble.\n",
    "\n",
    "Let's first try to understand what the bootstrap method is. Consider the case where you have 3 balls labeled A, B, and C. A bootstrap sample is a sample drawn from this with replacement. By replacement, we mean that any ball can be drawn many times.\n",
    "![Bootstrap](assets/bootstrap.png)\n",
    "For example, in the first bootstrap sample shown above in the diagram, B was drawn 3 times in a raw. In the second bootstrap sample, A was drwan two times while B was drawn once, and so on. Now, how bootstrap can help us produce an ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging: Training**\n",
    "\n",
    "In the training phase, bagging consists of drawing N different bootstrap samples from the training set. As shown in the diagram below, each of these bootstrap samples are the used to train N models that use the same algorithm:\n",
    "![Bootstrap Training](assets/bootstrap_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging: Prediction**\n",
    "\n",
    "When a new instance is fed to the different models, forming the bagging ensemble, each model output its prediction:\n",
    "![Bagging Prediction](assets/bagging_prediction.png)\n",
    "The meta-model collects these predictions and outputs a final prediction depending on the nature of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging: Classification & Regression**\n",
    "\n",
    "* In *classification*, the final prediction is obtained by majority voting. The corresponding classifier in scikit-learn is `BaggingClassifier` \n",
    "* In *regression*, the final prediction is the average of the predictions made by the individual models forming the ensemble. The corresponding regressor in scikit-learn is `BaggingRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the bagging classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples we'll work with the Indian Liver Patient dataset from the UCI machine learning repository. Our task is to predict whether a patient suffers from a liver disease using 10 features including Albumin, age and gender. We'll do so using a Bagging Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.824143Z",
     "start_time": "2020-02-11T09:27:32.817573Z"
    }
   },
   "outputs": [],
   "source": [
    "X = utils.liver_disease.drop('Liver_disease', axis=1)\n",
    "y = utils.liver_disease['Liver_disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.829539Z",
     "start_time": "2020-02-11T09:27:32.826687Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.833217Z",
     "start_time": "2020-02-11T09:27:32.830966Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.837467Z",
     "start_time": "2020-02-11T09:27:32.834518Z"
    }
   },
   "outputs": [],
   "source": [
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Bagging performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we instantiated the bagging classifier, it's time to train it and evaluate its test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.953765Z",
     "start_time": "2020-02-11T09:27:32.838933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort=False,\n",
       "                                                        random_state=1,\n",
       "                                                        splitter='best'),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=50, n_jobs=None,\n",
       "                  oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:32.964841Z",
     "start_time": "2020-02-11T09:27:32.955327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:56.819713Z",
     "start_time": "2020-02-11T09:27:56.816474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "print(f'Test set accuracy of bc: {acc_test:.2f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in bagging, some instances may be sampled several times for one model. On the other hand, other instance may not be sampled at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Out Of Bag (OOB) instances**\n",
    "\n",
    "On average, for each model, 63% of the training instances are sampled. The remaining 37% that are not sampled constitute what is known as the Out-of-bag or OOB instances. Since OOB instances are not seen by a model during training, these can be used to estimate the performance of the ensemble without the need for cross-validation. This technique is known as OOB-evaluation.\n",
    "\n",
    "To understand OOB-evaluation more concretely, take a look at this diagram:\n",
    "![OOB Evaluation](assets/oob_evaluation.png)\n",
    "Here, for each model, the bootstrap instances are shown in blue while the OOB-instances are shown in red. Each of the N models constituting the ensemble is then trained on its corresponding bootstrap samples and evaluated on the OOB instances. This leads to the obtainment of `N` OOB scores labeled $OOB_1$ to $OOB_N$. The OOB-score of the bagging ensemble is evaluated as the average of these N OOB scores as shown by the formula on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the ground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we'll compare the OOB accuracy to the test set accuracy of a bagging classifier trained on the Indian Liver Patient dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, we can evaluate the OOB accuracy of an ensemble classifier by setting the parameter `oob_score` to `True` during instantiation. After training the classifier, the OOB accuracy can be obtained by accessing the `.oob_score_` attribute from the corresponding instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:32:28.250344Z",
     "start_time": "2020-02-11T09:32:28.247752Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:32:40.027398Z",
     "start_time": "2020-02-11T09:32:40.024831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:33:08.533968Z",
     "start_time": "2020-02-11T09:33:08.530809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, oob_score=True, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOB Score vs Test Set Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we instantiated bc, you will fit it to the training set and evaluate its test set and OOB accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:34:43.777279Z",
     "start_time": "2020-02-11T09:34:43.658988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=8,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort=False,\n",
       "                                                        random_state=1,\n",
       "                                                        splitter='best'),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=50, n_jobs=None, oob_score=True,\n",
       "                  random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:34:50.536759Z",
     "start_time": "2020-02-11T09:34:50.528185Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:35:00.505559Z",
     "start_time": "2020-02-11T09:35:00.502510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:35:07.968539Z",
     "start_time": "2020-02-11T09:35:07.966043Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:35:36.849639Z",
     "start_time": "2020-02-11T09:35:36.846768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.658, OOB accuracy: 0.715\n"
     ]
    }
   ],
   "source": [
    "# Print acc_test and acc_oob\n",
    "print(f'Test set accuracy: {acc_test:.3f}, OOB accuracy: {acc_oob:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn now about another ensemble learning method knows as `Random Forests`. Recall that in bagging the base estimator could be any model, including a decision tree, logistics regression or even a neural network. Each estimator is trained on a distinct bootstrap sample drawn from the training set using all available features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests is an ensemble method that uses a decision tree as a base estimator. In Random Forests, each estimator is trained on a different bootstrap sample having the same size as the training set. Random Forests introduces further randomization than bagging when training each of the base estimator. When each tree is trained, only $d$ features can be sampled at each node without replacement, where $d$ is a number smaller than the total number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below shows the training procedure for random forests:\n",
    "\n",
    "![RF Training](assets/rf_training.png)\n",
    "\n",
    "Notice how each tree forming the ensemble is trained on a different bootstrap sample from the training set. In addition, when a tree is trained, at each node, only $d$ features are sampled from all features without replacement. The node is then split using the sampled feature that maximizes information gain. In scikit-learn $d$ defaults to the square-root of the number of features. For example, if there are 100 features, only 10 features are sampled at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained, prediction can be made on new instances. When a new instance is fed to the different base estimators, each of them outputs a prediction. The predictions are then collected by the random forests meta-model and a final prediction is made depending on the nature of the problem.\n",
    "\n",
    "![RF Prediction](assets/rf_prediction.png)\n",
    "\n",
    "* For `classification`, the final prediction is made by majority voting. The corresponding scikit-learn class is `RandomForestClassifier`.\n",
    "* For `regression`, the final prediction is the average of all the labels predicted by the base estimators. The corresponding scikit-learn class is `RandomForestRegressor`.\n",
    "\n",
    "In general, Random Forests achieves a lower variance than individual trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a tree based method is trained, the predictive power of a feature or its importance can be assessed. In scikit-learn, feature importance is assessed by measuring how much the tree nodes use a particular feature to reduce impurity. Note that the importance of a feature is expressed as a percentage indicating the weight of that feature in training and prediction. \n",
    "\n",
    "Once we train a tree-based model in scikit-learn, the features importances can be accessed by extracting the `feature_importance_` attribute from the model. To visualize the importance of features as assessed by `rf`, we can create a pandas series of the features importances and then sort the series and make a horizontal-barplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a RF regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples we'll predict bike rental demand in the Capital Bikeshare program in Washington, D.C using historical weather data from the [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) dataset available through Kaggle. For this purpose, you will be using the random forests algorithm. As a first step, we'll define a random forests regressor and fit it to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:38:20.237539Z",
     "start_time": "2020-02-11T09:38:20.214938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "      <th>instant</th>\n",
       "      <th>mnth</th>\n",
       "      <th>yr</th>\n",
       "      <th>Clear to partly cloudy</th>\n",
       "      <th>Light Precipitation</th>\n",
       "      <th>Misty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>149</td>\n",
       "      <td>13004</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>93</td>\n",
       "      <td>13005</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>90</td>\n",
       "      <td>13006</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>33</td>\n",
       "      <td>13007</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>4</td>\n",
       "      <td>13008</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hr  holiday  workingday  temp   hum  windspeed  cnt  instant  mnth  yr  \\\n",
       "0   0        0           0  0.76  0.66     0.0000  149    13004     7   1   \n",
       "1   1        0           0  0.74  0.70     0.1343   93    13005     7   1   \n",
       "2   2        0           0  0.72  0.74     0.0896   90    13006     7   1   \n",
       "3   3        0           0  0.72  0.84     0.1343   33    13007     7   1   \n",
       "4   4        0           0  0.70  0.79     0.1940    4    13008     7   1   \n",
       "\n",
       "   Clear to partly cloudy  Light Precipitation  Misty  \n",
       "0                       1                    0      0  \n",
       "1                       1                    0      0  \n",
       "2                       1                    0      0  \n",
       "3                       1                    0      0  \n",
       "4                       1                    0      0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes = pd.read_csv('data/bikes.csv')\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:40:24.465301Z",
     "start_time": "2020-02-11T09:40:24.459760Z"
    }
   },
   "outputs": [],
   "source": [
    "X = bikes.drop('cnt', axis=1)\n",
    "y = bikes['cnt']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:45:17.710138Z",
     "start_time": "2020-02-11T09:45:17.707689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:45:19.739290Z",
     "start_time": "2020-02-11T09:45:19.736771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:46:23.865966Z",
     "start_time": "2020-02-11T09:46:23.769886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=25,\n",
       "                      n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the RF regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now evaluate the test set RMSE of the random forests regressor `rf` that we trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:49:15.206186Z",
     "start_time": "2020-02-11T09:49:15.203666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:49:23.478910Z",
     "start_time": "2020-02-11T09:49:23.471907Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:49:29.977554Z",
     "start_time": "2020-02-11T09:49:29.974624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:49:49.590541Z",
     "start_time": "2020-02-11T09:49:49.587931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 52.38\n"
     ]
    }
   ],
   "source": [
    "# Print rmse_test\n",
    "print(f'Test set RMSE of rf: {rmse_test:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing features importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll determine which features were the most predictive according to the random forests regressor `rf` that we trained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this purpose, you'll draw a horizontal barplot of the feature importance as assessed by `rf`. Fortunately, this can be done easily thanks to plotting capabilities of `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:52:02.374517Z",
     "start_time": "2020-02-11T09:52:02.369340Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rf.feature_importances_, index= X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:53:03.925363Z",
     "start_time": "2020-02-11T09:53:03.922038Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:53:12.021030Z",
     "start_time": "2020-02-11T09:53:11.753871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh', color='lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, `hr` and `workingday` are the most important features according to `rf`. The importances of these two features add up to more than 90%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting refers to an ensemble method in which several models are trained sequentially with each model learning from the errors of its predecessors. In this chapter, we'll introduce two boosting methods: AdaBoost and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting** refers to an ensemble method in which many predictors are trained and each predictor learns from the errors of its predecessor. More formally, in boosting many weak learners are combined to form a strong learner. A **weak learner** is a model doing slightly better than random guessing. For example, a decision tree with a maximum-depth of one, known as decision-stump, is a weak learner.\n",
    "\n",
    "In boosting, an ensemble of predictors are trained sequentially and each predictor tries to correct the errors made by its predecessor. The two boosting methods we'll explore in this course are `AdaBoost` and `GradientBoosting`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost stands for **Ada**ptive **Boost**ing. In AdaBoost, each predictor pays more attention to the instances wrongly predicted by its predecessor by constantly changing the weights of training instances. Further more, each predictor is assigned a coefficient $\\alpha$ that weighs its contribution in the ensemble's final prediction ($\\alpha$ depends on the predictor's training error).\n",
    "\n",
    "As shown in the diagram, there are N predictors in total:\n",
    "\n",
    "![AdaBoost Training](assets/adaboost.png)\n",
    "\n",
    "First, $predictor_1$ is trained on the initial dataset (X,y), and the training error for $predictor_1$ is determined. This error can be then be used to determine the $\\alpha_1$ which is $predictor_1$'s coefficient. $\\alpha_1$ is then used to determine the weights $W^{(2)}$ of the training instances for $predictor_2$. Notice how the incorrectly predicted instances shown in green acquire higher weights. When the weighted instances are used to train $predictor_2$, this predictor is forced to pay more attention to the incorrectly predicted instances. This process is repeated sequentially, until the $N$ predictors forming the ensemble are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important parameter used in training is the learning rate: $0 < \\eta < 1$; it is used to shrink the coefficient $\\alpha$ of a trained predictor. It's important to note that there's a trade-off between $\\eta$ and the number of estimators. A smaller value of $\\eta$ should be compensated by a greater number of estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the predictors in the ensemble are trained, the label of a new instance can be predicted depending on the nature of the problem. \n",
    "\n",
    "* For `classification`, each predictor predicts the label of the new instance and the ensemble's prediction is obtained by weighted majority voting. The scikit-learn class is `AdaBoostClassifier`.\n",
    "* For `regression`, the same procedure is applied and the ensemble's prediction is obtained by performing a weighted average. The scikit-learn class is `AdaBoostRegressor`.\n",
    "\n",
    "It's important to note that individual predictors need to be CARTs. However CARTs are used most of the time in boosting because of their high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples we'll revisit the [Indian Liver Patient](https://www.kaggle.com/uciml/indian-liver-patient-records) dataset which was introduced in a previous chapter. Our task is to predict whether a patient suffers from a liver disease using 10 features including Albumin, age and gender. However, this time, we'll be training an AdaBoost ensemble to perform the classification task. In addition, given that this dataset is imbalanced, we'll be using the ROC AUC score as a metric instead of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we'll start by instantiating an AdaBoost classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:32:16.532778Z",
     "start_time": "2020-02-11T11:32:16.530395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:33:00.432997Z",
     "start_time": "2020-02-11T11:33:00.430146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:33:11.115983Z",
     "start_time": "2020-02-11T11:33:11.113292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've instantiated the AdaBoost classifier `ada`, it's time train it. We will also predict the probabilities of obtaining the positive class in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the classifier `ada` is trained, call the `.predict_proba()` method by passing `X_test` as a parameter and extract these probabilities by slicing all the values in the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:42:36.694254Z",
     "start_time": "2020-02-11T11:42:36.687621Z"
    }
   },
   "outputs": [],
   "source": [
    "X = utils.liver_disease.drop('Liver_disease', axis=1)\n",
    "y = utils.liver_disease['Liver_disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:43:35.756265Z",
     "start_time": "2020-02-11T11:43:35.471971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=2,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort=False,\n",
       "                                                         random_state=1,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=1.0, n_estimators=180, random_state=1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit ada to the training set\n",
    "ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:43:43.676251Z",
     "start_time": "2020-02-11T11:43:43.646416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done training `ada` and predicting the probabilities of obtaining the positive class in the test set, it's time to evaluate `ada`'s ROC AUC score. Recall that the ROC AUC score of a binary classifier can be determined using the `roc_auc_score()` function from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:46:39.929255Z",
     "start_time": "2020-02-11T11:46:39.926812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:46:46.910352Z",
     "start_time": "2020-02-11T11:46:46.904989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:47:07.115150Z",
     "start_time": "2020-02-11T11:47:07.112399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Print roc_auc_score\n",
    "print(f'ROC AUC score: {ada_roc_auc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! This untuned AdaBoost classifier achieved a ROC AUC score of 0.64!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is a popular boosting algorithm that has a proven track record of winning many machine learning competitions. In gradient boosting, each predictor in the ensemble corrects its predecessor's error. In contrast to AdaBoost, the weights of the training instances are not tweaked. Instead, each predictor is trained using the residual errors of its predecessor as labels. In the following sections, we'll explore the technique  known as gradient boosted trees where the base learner is a CART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how gradient boosted trees are trained for a regression problem, take a look at the diagram here:\n",
    "\n",
    "![Gradient Boosting](assets/boosted_trees.png)\n",
    "\n",
    "The ensemble consists on $N$ trees. $Tree_1$ is trained using the features matrix $X$ and the dataset labels $y$. The predictions labeled $\\hat{y}_1$ are used to determine the training set residual errors $r_1$. $Tree_2$ is then trained using the features matrix $X$ and the residual errors $r_1$ of $Tree_1$ as labels. The predicted residuals $\\hat{r}_1$ are then used to determine the residuals of residuals which are labeled $r_2$. This process is repeated until all of the $N$ trees forming the ensemble are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important parameter used in training gradient boosted trees is `shrinkage`. In this context, shrinkage refers to the fact that the prediction of each tree in the ensemble is shrinked after it is multiplied by a learning rate $\\eta$ which is a number between 0 and 1.\n",
    "\n",
    "Similar to AdaBoost, there's a trade-off between $\\eta$ and the number of estimators. Decreasing the learning rate needs to be compensated by increasing the number of estimators in order for the ensemble to reach a certain performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all trees in the ensemble are trained, prediction can be made. When a new instance is available, each tree predicts a label and the final ensemble prediction is given by the formula shown below:\n",
    "\n",
    "$$ y_{pred} = y_1 + \\eta r_1 + ... + \\eta r_N $$\n",
    "\n",
    "In scikit-learn, the class for a gradient boosting regressor is `GradientBoostingRegressor`. Though not discussed in this course, a similar algorithm is used for classification problems. The class implementing gradient-boosting-classification in scikit-learn is `GradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GB regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now revisit the [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) dataset that was introduced in the previous chapter. Recall that our task is to predict the bike rental demand using historical weather data from the Capital Bikeshare program in Washington, D.C.. For this purpose, we'll be using a gradient boosting regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we'll start by instantiating a gradient boosting regressor which we will train in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:54:43.850345Z",
     "start_time": "2020-02-11T11:54:43.848220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:55:06.626867Z",
     "start_time": "2020-02-11T11:55:06.624155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4, n_estimators=200, random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the GB regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now train the gradient boosting regressor `gb` that we instantiated in the previous example and predict test set labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:56:43.906948Z",
     "start_time": "2020-02-11T11:56:43.901085Z"
    }
   },
   "outputs": [],
   "source": [
    "X = bikes.drop('cnt', axis=1)\n",
    "y = bikes['cnt']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:57:08.336156Z",
     "start_time": "2020-02-11T11:57:08.174176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=4,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                          n_iter_no_change=None, presort='auto', random_state=1,\n",
       "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit gb to the training set\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:57:16.838494Z",
     "start_time": "2020-02-11T11:57:16.833637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the GB regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the test set predictions are available, we can use them to evaluate the test set Root Mean Squared Error (RMSE) of `gb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:59:11.545732Z",
     "start_time": "2020-02-11T11:59:11.542946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:00:21.753483Z",
     "start_time": "2020-02-11T12:00:21.750399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute MSE\n",
    "mse_test = MSE(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:00:31.688619Z",
     "start_time": "2020-02-11T12:00:31.686190Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute RMSE\n",
    "rmse_test = mse_test ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:00:54.978832Z",
     "start_time": "2020-02-11T12:00:54.975955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of gb: 43.140\n"
     ]
    }
   ],
   "source": [
    "# Print RMSE\n",
    "print(f'Test set RMSE of gb: {rmse_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting (SGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting involves an exhaustive search procedure. Each tree in the ensemble is trained to find the best split-points and the best features. This procedure may lead to CARTs that use the same split-points and possibly the same features. To mitigate these effects, we can use an algorithm known as stochastic gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stochastic gradient boosting, each CART is trained on a random subset of the training data. This subset is sampled without replacement. Further more, at the level of each node, features are sampled without replacement when choosing the best split-points. As a result, this creates further diversity in the ensemble and the net effect is adding more variance to the ensemble of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochasting Gradient Boosting: Training**\n",
    "\n",
    "Let's take a closer look at the training procedure used in stochastic gradient boosting by examining the diagram shown below:\n",
    "\n",
    "![SGB Training](assets/sgb_training.png)\n",
    "\n",
    "First, instead of providing all the training instances to a tree, only a fraction of these instances are provided through sampling without relacement. The sample data is then used for training a tree. However, not all features are considered when a split is made. Instead, only a certain randomly sampled fraction of these features are used for this purpose.\n",
    "\n",
    "Once a tree is trained, predictions are made and the residual errors can be computed. These residual errors are multiplied by the learning rate $\\eta$ ad are fed to the next tree in the ensemble. This procedure is repeated seqentially until all the trees in the ensemble are trained.\n",
    "\n",
    "The prediction procedure for a new instance in stochastic gradient boosting is similar to that of gradient boosting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with SGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the examples from the previous lesson, you'll be working with the Bike Sharing Demand dataset. In the following set of examples, we'll solve this bike count regression problem using stochastic gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:03:27.267504Z",
     "start_time": "2020-02-11T12:03:27.265412Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:03:54.005623Z",
     "start_time": "2020-02-11T12:03:54.002419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(max_depth=4, \n",
    "                                 subsample=0.9,\n",
    "                                 max_features=0.75,\n",
    "                                 n_estimators=200,                                \n",
    "                                 random_state=utils.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the SGB regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll train the SGBR `sgbr` instantiated above and predict the test set labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:05:03.994733Z",
     "start_time": "2020-02-11T12:05:03.819462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=4,\n",
       "                          max_features=0.75, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                          n_iter_no_change=None, presort='auto', random_state=1,\n",
       "                          subsample=0.9, tol=0.0001, validation_fraction=0.1,\n",
       "                          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:05:14.298817Z",
     "start_time": "2020-02-11T12:05:14.293719Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the SGB regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared the ground to determine the test set RMSE of `sgbr` which you shall evaluate in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:07:02.137403Z",
     "start_time": "2020-02-11T12:07:02.133176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of sgbr: 42.183\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test ** 0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic gradient boosting regressor achieves a lower test set RMSE than the gradient boosting regressor (which was 43.140)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
